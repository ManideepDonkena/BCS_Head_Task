{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaading data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_eval = pd.read_csv('eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2061, 3), (9000, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\D SAIPAVAN\n",
      "[nltk_data]     KUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom excellent meeting app \n",
      " zoom is an excellent meeting app.\n"
     ]
    }
   ],
   "source": [
    "# removing the stop words helps to decrease the computation time\n",
    "text = df_train['text'][6]\n",
    "print(preprocess_text(text),'\\n',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we have preprocessed text\n",
    "# we can see that in training data there are only positive samples \n",
    "# so we need to balance the data by adding negative samples\n",
    "# for adding negative samples by randomly selecting a text and adding current reason \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocessing to the text in df_train\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_train['reason'] = df_train['reason'].apply(preprocess_text)\n",
    "df_eval['text'] = df_eval['text'].apply(preprocess_text)\n",
    "df_eval['reason'] = df_eval['reason'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "model = SentenceTransformer('stsb-roberta-large') #roberta is a transformer model(base model) which is trained on a large corpus of text for the task of sentence similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "for idx,row in df_train.iterrows(): \n",
    "        inp_example = {\n",
    "                \"texts\" :row['text'],\n",
    "                'reason': row['reason'], \n",
    "                'label': (row['label'])\n",
    "                       }\n",
    "        train_list.append(inp_example)\n",
    "test_list = []\n",
    "for idx,row in df_eval.iterrows():  \n",
    "        inp_example = inp_example = {\n",
    "                \"texts\" :row['text'],\n",
    "                'reason': row['reason'], \n",
    "                'label': row['label']\n",
    "                       }\n",
    "        test_list.append(inp_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6732]]) 1 1\n",
      "tensor([[0.7836]]) 0 1\n",
      "tensor([[0.2829]]) 0 0\n",
      "tensor([[0.3715]]) 0 0\n",
      "tensor([[0.4413]]) 0 0\n",
      "tensor([[0.4727]]) 1 0\n",
      "tensor([[0.0450]]) 0 0\n",
      "tensor([[0.6454]]) 1 1\n",
      "tensor([[0.3686]]) 0 0\n",
      "tensor([[0.3925]]) 0 0\n",
      "tensor([[0.2735]]) 0 0\n",
      "tensor([[0.4236]]) 0 0\n",
      "tensor([[0.4069]]) 1 0\n",
      "tensor([[0.2796]]) 1 0\n",
      "tensor([[0.4030]]) 0 0\n",
      "tensor([[0.2157]]) 1 0\n",
      "tensor([[0.4821]]) 1 0\n",
      "tensor([[0.3553]]) 0 0\n",
      "tensor([[0.5774]]) 0 1\n",
      "tensor([[0.5297]]) 0 1\n",
      "tensor([[0.4579]]) 0 0\n",
      "tensor([[0.1927]]) 0 0\n",
      "tensor([[0.6561]]) 0 1\n",
      "tensor([[0.2074]]) 0 0\n",
      "tensor([[0.0943]]) 0 0\n",
      "tensor([[0.4823]]) 1 0\n",
      "tensor([[0.8109]]) 1 1\n",
      "tensor([[0.6028]]) 1 1\n",
      "tensor([[0.2562]]) 0 0\n",
      "tensor([[0.4216]]) 0 0\n",
      "tensor([[0.5522]]) 0 1\n",
      "tensor([[0.2215]]) 1 0\n",
      "tensor([[0.2572]]) 0 0\n",
      "tensor([[0.1025]]) 1 0\n",
      "tensor([[0.3471]]) 1 0\n",
      "tensor([[0.3109]]) 0 0\n",
      "tensor([[0.4582]]) 0 0\n",
      "tensor([[0.5147]]) 1 1\n",
      "tensor([[0.3477]]) 0 0\n",
      "tensor([[0.1528]]) 0 0\n",
      "tensor([[0.5863]]) 0 1\n",
      "tensor([[0.1387]]) 0 0\n",
      "tensor([[0.3573]]) 0 0\n",
      "tensor([[0.7096]]) 1 1\n",
      "tensor([[0.5548]]) 0 1\n",
      "tensor([[0.5627]]) 0 1\n",
      "tensor([[0.5013]]) 1 1\n",
      "tensor([[0.3263]]) 0 0\n",
      "tensor([[0.2889]]) 0 0\n",
      "tensor([[0.4328]]) 1 0\n",
      "tensor([[0.4709]]) 0 0\n",
      "tensor([[0.7411]]) 0 1\n",
      "tensor([[0.2700]]) 0 0\n",
      "tensor([[0.3719]]) 0 0\n",
      "tensor([[0.3479]]) 1 0\n",
      "tensor([[0.4334]]) 0 0\n",
      "tensor([[0.2901]]) 0 0\n",
      "tensor([[0.5649]]) 1 1\n",
      "tensor([[0.3774]]) 1 0\n",
      "tensor([[0.2272]]) 0 0\n",
      "tensor([[0.4368]]) 0 0\n",
      "tensor([[0.5147]]) 1 1\n",
      "tensor([[0.4050]]) 1 0\n",
      "tensor([[0.7144]]) 1 1\n",
      "tensor([[0.8348]]) 1 1\n",
      "tensor([[0.5144]]) 1 1\n",
      "tensor([[0.7478]]) 1 1\n",
      "tensor([[0.3561]]) 0 0\n",
      "tensor([[0.4341]]) 0 0\n",
      "tensor([[0.4999]]) 1 0\n",
      "tensor([[0.1945]]) 0 0\n",
      "tensor([[0.0604]]) 1 0\n",
      "tensor([[0.2181]]) 1 0\n",
      "tensor([[0.5120]]) 0 1\n",
      "tensor([[0.1685]]) 0 0\n",
      "tensor([[0.4129]]) 1 0\n",
      "tensor([[0.4196]]) 0 0\n",
      "tensor([[0.5822]]) 0 1\n",
      "tensor([[0.2939]]) 1 0\n",
      "tensor([[0.1506]]) 0 0\n",
      "tensor([[0.0943]]) 0 0\n",
      "tensor([[0.5152]]) 0 1\n",
      "tensor([[0.7577]]) 1 1\n",
      "tensor([[0.4176]]) 1 0\n",
      "tensor([[0.3085]]) 0 0\n",
      "tensor([[0.6502]]) 0 1\n",
      "tensor([[0.4782]]) 0 0\n",
      "tensor([[0.7294]]) 1 1\n",
      "tensor([[0.4347]]) 0 0\n",
      "tensor([[0.0140]]) 0 0\n",
      "tensor([[0.4681]]) 0 0\n",
      "tensor([[0.5553]]) 1 1\n",
      "tensor([[0.4581]]) 1 0\n",
      "tensor([[0.5619]]) 0 1\n",
      "tensor([[0.2861]]) 0 0\n",
      "tensor([[0.4158]]) 0 0\n",
      "tensor([[0.5776]]) 1 1\n",
      "tensor([[0.8416]]) 0 1\n",
      "tensor([[0.2075]]) 1 0\n",
      "tensor([[0.4968]]) 0 0\n",
      "tensor([[0.4150]]) 1 0\n",
      "tensor([[0.5130]]) 1 1\n",
      "tensor([[0.6305]]) 1 1\n",
      "tensor([[0.2084]]) 0 0\n",
      "tensor([[0.4723]]) 0 0\n",
      "tensor([[0.4727]]) 1 0\n",
      "tensor([[0.2858]]) 0 0\n",
      "tensor([[0.3030]]) 0 0\n",
      "tensor([[0.2807]]) 1 0\n",
      "tensor([[0.4377]]) 0 0\n",
      "tensor([[0.2406]]) 0 0\n",
      "tensor([[0.5693]]) 0 1\n",
      "tensor([[0.3783]]) 1 0\n",
      "tensor([[0.4737]]) 0 0\n",
      "tensor([[0.3395]]) 0 0\n",
      "tensor([[0.3143]]) 0 0\n",
      "tensor([[-0.0028]]) 1 0\n",
      "tensor([[0.4895]]) 0 0\n",
      "tensor([[0.2911]]) 0 0\n",
      "tensor([[0.3710]]) 0 0\n",
      "tensor([[0.3123]]) 1 0\n",
      "tensor([[0.5220]]) 1 1\n",
      "tensor([[0.5978]]) 1 1\n",
      "tensor([[0.1757]]) 0 0\n",
      "tensor([[0.5668]]) 1 1\n",
      "tensor([[0.4848]]) 0 0\n",
      "tensor([[0.2177]]) 1 0\n",
      "tensor([[0.5291]]) 1 1\n",
      "tensor([[0.4834]]) 1 0\n",
      "tensor([[0.3850]]) 1 0\n",
      "tensor([[0.1419]]) 1 0\n",
      "tensor([[0.2841]]) 0 0\n",
      "tensor([[0.4166]]) 0 0\n",
      "tensor([[0.5444]]) 1 1\n",
      "tensor([[0.5300]]) 1 1\n",
      "tensor([[0.8439]]) 1 1\n",
      "tensor([[0.4857]]) 0 0\n",
      "tensor([[0.2209]]) 0 0\n",
      "tensor([[0.5772]]) 0 1\n",
      "tensor([[0.4250]]) 0 0\n",
      "tensor([[0.4560]]) 0 0\n",
      "tensor([[0.0849]]) 0 0\n",
      "tensor([[0.4745]]) 0 0\n",
      "tensor([[0.1854]]) 0 0\n",
      "tensor([[0.1343]]) 0 0\n",
      "tensor([[0.2531]]) 0 0\n",
      "tensor([[0.4657]]) 0 0\n",
      "tensor([[0.3409]]) 1 0\n",
      "tensor([[0.0255]]) 0 0\n",
      "tensor([[0.1866]]) 0 0\n",
      "tensor([[0.6691]]) 1 1\n",
      "tensor([[0.3518]]) 0 0\n",
      "tensor([[0.1880]]) 0 0\n",
      "tensor([[0.2573]]) 0 0\n",
      "tensor([[0.2609]]) 0 0\n",
      "tensor([[0.1743]]) 0 0\n",
      "tensor([[0.6504]]) 1 1\n",
      "tensor([[0.6542]]) 1 1\n",
      "tensor([[0.7034]]) 1 1\n",
      "tensor([[0.2049]]) 0 0\n",
      "tensor([[0.3696]]) 0 0\n",
      "tensor([[0.2505]]) 0 0\n",
      "tensor([[0.4546]]) 0 0\n",
      "tensor([[0.5047]]) 1 1\n",
      "tensor([[0.5072]]) 0 1\n",
      "tensor([[0.7101]]) 1 1\n",
      "tensor([[0.3942]]) 0 0\n",
      "tensor([[0.6123]]) 0 1\n",
      "tensor([[0.6417]]) 0 1\n",
      "tensor([[0.7684]]) 1 1\n",
      "tensor([[0.3861]]) 0 0\n",
      "tensor([[0.6505]]) 0 1\n",
      "tensor([[0.8285]]) 1 1\n",
      "tensor([[0.6779]]) 1 1\n",
      "tensor([[0.4765]]) 1 0\n",
      "tensor([[0.3174]]) 0 0\n",
      "tensor([[0.5710]]) 0 1\n",
      "tensor([[0.2173]]) 0 0\n",
      "tensor([[0.3405]]) 1 0\n",
      "tensor([[0.4543]]) 0 0\n",
      "tensor([[0.6608]]) 0 1\n",
      "tensor([[0.4152]]) 1 0\n",
      "tensor([[0.0609]]) 0 0\n",
      "tensor([[0.4490]]) 1 0\n",
      "tensor([[0.4496]]) 1 0\n",
      "tensor([[0.5453]]) 1 1\n",
      "tensor([[0.6092]]) 0 1\n",
      "tensor([[0.3056]]) 0 0\n",
      "tensor([[0.5350]]) 0 1\n",
      "tensor([[0.5746]]) 0 1\n",
      "tensor([[0.3255]]) 1 0\n",
      "tensor([[0.2829]]) 0 0\n",
      "tensor([[0.2295]]) 0 0\n",
      "tensor([[0.7450]]) 1 1\n",
      "tensor([[0.4008]]) 1 0\n",
      "tensor([[0.4219]]) 0 0\n",
      "tensor([[0.1064]]) 0 0\n",
      "tensor([[0.3321]]) 1 0\n",
      "tensor([[0.2562]]) 0 0\n",
      "tensor([[0.8219]]) 1 1\n",
      "tensor([[0.2528]]) 1 0\n",
      "tensor([[0.4148]]) 0 0\n",
      "tensor([[0.6212]]) 1 1\n",
      "tensor([[0.4524]]) 1 0\n",
      "tensor([[0.2880]]) 1 0\n",
      "tensor([[0.2369]]) 1 0\n",
      "tensor([[0.7214]]) 0 1\n",
      "tensor([[0.4381]]) 0 0\n",
      "tensor([[0.7075]]) 1 1\n",
      "tensor([[0.2824]]) 0 0\n",
      "tensor([[0.3643]]) 0 0\n",
      "tensor([[0.3244]]) 0 0\n",
      "tensor([[0.2748]]) 1 0\n",
      "tensor([[0.4623]]) 0 0\n",
      "tensor([[0.7595]]) 1 1\n",
      "tensor([[0.2466]]) 0 0\n",
      "tensor([[0.6777]]) 1 1\n",
      "tensor([[0.3688]]) 0 0\n",
      "tensor([[0.5582]]) 0 1\n",
      "tensor([[0.3712]]) 0 0\n",
      "tensor([[0.4680]]) 1 0\n",
      "tensor([[0.8150]]) 1 1\n",
      "tensor([[0.1687]]) 0 0\n",
      "tensor([[0.5956]]) 1 1\n",
      "tensor([[0.4066]]) 0 0\n",
      "tensor([[0.1555]]) 1 0\n",
      "tensor([[0.2403]]) 0 0\n",
      "tensor([[0.4685]]) 1 0\n",
      "tensor([[0.3343]]) 0 0\n",
      "tensor([[0.4884]]) 0 0\n",
      "tensor([[0.1559]]) 0 0\n",
      "tensor([[0.2758]]) 0 0\n",
      "tensor([[0.2409]]) 0 0\n",
      "tensor([[0.5295]]) 1 1\n",
      "tensor([[0.6647]]) 0 1\n",
      "tensor([[0.0783]]) 0 0\n",
      "tensor([[0.1257]]) 0 0\n",
      "tensor([[0.6755]]) 1 1\n",
      "tensor([[0.5032]]) 0 1\n",
      "tensor([[0.3414]]) 0 0\n",
      "tensor([[0.2944]]) 1 0\n",
      "tensor([[0.3034]]) 0 0\n",
      "tensor([[0.5077]]) 1 1\n",
      "tensor([[0.4304]]) 0 0\n",
      "tensor([[0.3989]]) 1 0\n",
      "tensor([[0.1913]]) 0 0\n",
      "tensor([[0.6177]]) 0 1\n",
      "tensor([[0.6619]]) 0 1\n",
      "tensor([[0.3507]]) 1 0\n",
      "tensor([[0.4543]]) 1 0\n",
      "tensor([[0.3679]]) 0 0\n",
      "tensor([[0.3029]]) 0 0\n",
      "tensor([[0.2744]]) 1 0\n",
      "tensor([[0.1851]]) 0 0\n",
      "tensor([[0.4438]]) 1 0\n",
      "tensor([[0.3908]]) 0 0\n",
      "tensor([[0.7743]]) 1 1\n",
      "tensor([[0.4055]]) 1 0\n",
      "tensor([[0.5565]]) 0 1\n",
      "tensor([[0.1837]]) 0 0\n",
      "tensor([[0.5133]]) 1 1\n",
      "tensor([[0.9138]]) 1 1\n",
      "tensor([[0.5675]]) 1 1\n",
      "tensor([[0.3221]]) 0 0\n",
      "tensor([[0.0482]]) 0 0\n",
      "tensor([[0.3903]]) 1 0\n",
      "tensor([[0.5097]]) 0 1\n",
      "tensor([[0.7038]]) 0 1\n",
      "tensor([[0.2727]]) 1 0\n",
      "tensor([[0.3025]]) 0 0\n",
      "tensor([[0.5425]]) 1 1\n",
      "tensor([[0.1688]]) 0 0\n",
      "tensor([[0.5417]]) 0 1\n",
      "tensor([[0.2839]]) 0 0\n",
      "tensor([[0.3353]]) 0 0\n",
      "tensor([[0.7221]]) 0 1\n",
      "tensor([[0.2297]]) 1 0\n",
      "tensor([[0.2866]]) 0 0\n",
      "tensor([[0.1730]]) 0 0\n",
      "tensor([[0.2793]]) 1 0\n",
      "tensor([[0.2456]]) 0 0\n",
      "tensor([[0.3869]]) 1 0\n",
      "tensor([[0.5548]]) 0 1\n",
      "tensor([[0.6486]]) 1 1\n",
      "tensor([[0.2934]]) 1 0\n",
      "tensor([[0.7773]]) 1 1\n",
      "tensor([[0.4062]]) 0 0\n",
      "tensor([[0.4933]]) 0 0\n",
      "tensor([[0.4861]]) 0 0\n",
      "tensor([[0.4838]]) 0 0\n",
      "tensor([[0.0672]]) 0 0\n",
      "tensor([[0.4891]]) 0 0\n",
      "tensor([[0.2386]]) 0 0\n",
      "tensor([[0.5388]]) 0 1\n",
      "tensor([[0.2964]]) 0 0\n",
      "tensor([[0.4375]]) 0 0\n",
      "tensor([[0.5461]]) 0 1\n",
      "tensor([[0.4393]]) 1 0\n",
      "tensor([[0.5820]]) 0 1\n",
      "tensor([[0.2266]]) 1 0\n",
      "tensor([[0.1943]]) 1 0\n",
      "tensor([[0.8036]]) 1 1\n",
      "tensor([[0.3750]]) 0 0\n",
      "tensor([[0.3952]]) 1 0\n",
      "tensor([[0.3522]]) 1 0\n",
      "tensor([[0.0506]]) 0 0\n",
      "tensor([[0.3565]]) 0 0\n",
      "tensor([[0.4374]]) 0 0\n",
      "tensor([[0.4283]]) 1 0\n",
      "tensor([[0.3870]]) 0 0\n",
      "tensor([[0.2111]]) 0 0\n",
      "tensor([[0.4714]]) 1 0\n",
      "tensor([[0.4721]]) 0 0\n",
      "tensor([[0.5099]]) 0 1\n",
      "tensor([[0.4768]]) 0 0\n",
      "tensor([[0.0483]]) 0 0\n",
      "tensor([[0.5892]]) 1 1\n",
      "tensor([[-0.0586]]) 0 0\n",
      "tensor([[0.6362]]) 1 1\n",
      "tensor([[0.7229]]) 0 1\n",
      "tensor([[0.2815]]) 0 0\n",
      "tensor([[0.3828]]) 0 0\n",
      "tensor([[0.7261]]) 1 1\n",
      "tensor([[0.8247]]) 1 1\n",
      "tensor([[0.5884]]) 0 1\n",
      "tensor([[0.4727]]) 1 0\n",
      "tensor([[0.6153]]) 0 1\n",
      "tensor([[0.4424]]) 1 0\n",
      "tensor([[0.2564]]) 0 0\n",
      "tensor([[0.2533]]) 0 0\n",
      "tensor([[0.3681]]) 1 0\n",
      "tensor([[0.7901]]) 1 1\n",
      "tensor([[0.3245]]) 1 0\n",
      "tensor([[0.5791]]) 0 1\n",
      "tensor([[0.4728]]) 1 0\n",
      "tensor([[0.4182]]) 0 0\n",
      "tensor([[0.0778]]) 0 0\n",
      "tensor([[0.4550]]) 1 0\n",
      "tensor([[0.5690]]) 1 1\n",
      "tensor([[0.7653]]) 0 1\n",
      "tensor([[0.5548]]) 0 1\n",
      "tensor([[0.2146]]) 0 0\n",
      "tensor([[0.5032]]) 0 1\n",
      "tensor([[0.2785]]) 0 0\n",
      "tensor([[0.2917]]) 0 0\n",
      "tensor([[0.7832]]) 1 1\n",
      "tensor([[0.4630]]) 0 0\n",
      "tensor([[0.4042]]) 0 0\n",
      "tensor([[0.7661]]) 1 1\n",
      "tensor([[0.2263]]) 1 0\n",
      "tensor([[0.6422]]) 1 1\n",
      "tensor([[0.2667]]) 0 0\n",
      "tensor([[0.6156]]) 0 1\n",
      "tensor([[0.3707]]) 0 0\n",
      "tensor([[0.5229]]) 0 1\n",
      "tensor([[0.4381]]) 0 0\n",
      "tensor([[0.7285]]) 1 1\n",
      "tensor([[0.3597]]) 1 0\n",
      "tensor([[0.5237]]) 1 1\n",
      "tensor([[0.2550]]) 1 0\n",
      "tensor([[0.4180]]) 0 0\n",
      "tensor([[0.4159]]) 0 0\n",
      "tensor([[0.5023]]) 0 1\n",
      "tensor([[0.6297]]) 1 1\n",
      "tensor([[0.4324]]) 1 0\n",
      "tensor([[0.5672]]) 1 1\n",
      "tensor([[0.5292]]) 0 1\n",
      "tensor([[0.6046]]) 1 1\n",
      "tensor([[0.4794]]) 0 0\n",
      "tensor([[0.1387]]) 0 0\n",
      "tensor([[0.3324]]) 0 0\n",
      "tensor([[0.3906]]) 0 0\n",
      "tensor([[0.4227]]) 1 0\n",
      "tensor([[0.3738]]) 0 0\n",
      "tensor([[0.5129]]) 0 1\n",
      "tensor([[0.7235]]) 1 1\n",
      "tensor([[0.7319]]) 1 1\n",
      "tensor([[0.6549]]) 1 1\n",
      "tensor([[0.3623]]) 0 0\n",
      "tensor([[0.4758]]) 0 0\n",
      "tensor([[-0.0932]]) 0 0\n",
      "tensor([[0.3608]]) 0 0\n",
      "tensor([[0.5088]]) 0 1\n",
      "tensor([[0.3517]]) 0 0\n",
      "tensor([[0.4103]]) 0 0\n",
      "tensor([[0.0850]]) 0 0\n",
      "tensor([[0.3277]]) 0 0\n",
      "tensor([[0.6841]]) 1 1\n",
      "tensor([[0.2770]]) 0 0\n",
      "tensor([[0.1268]]) 0 0\n",
      "tensor([[0.3990]]) 1 0\n",
      "tensor([[0.2672]]) 0 0\n",
      "tensor([[0.0054]]) 1 0\n",
      "tensor([[0.4081]]) 1 0\n",
      "tensor([[0.5650]]) 1 1\n",
      "tensor([[0.4665]]) 0 0\n",
      "tensor([[0.2238]]) 0 0\n",
      "tensor([[0.5433]]) 0 1\n",
      "tensor([[0.1721]]) 0 0\n",
      "tensor([[0.7152]]) 1 1\n",
      "tensor([[0.1198]]) 1 0\n",
      "tensor([[0.5508]]) 0 1\n",
      "tensor([[0.3321]]) 0 0\n",
      "tensor([[0.4977]]) 0 0\n",
      "tensor([[0.2264]]) 0 0\n",
      "tensor([[0.1678]]) 0 0\n",
      "tensor([[0.4793]]) 0 0\n",
      "tensor([[0.6682]]) 0 1\n",
      "tensor([[0.4098]]) 0 0\n",
      "tensor([[0.1793]]) 1 0\n",
      "tensor([[0.5397]]) 1 1\n",
      "tensor([[0.5916]]) 0 1\n",
      "tensor([[0.5959]]) 1 1\n",
      "tensor([[0.5381]]) 1 1\n",
      "tensor([[0.2400]]) 0 0\n",
      "tensor([[0.7154]]) 0 1\n",
      "tensor([[0.7447]]) 1 1\n",
      "tensor([[0.7525]]) 1 1\n",
      "tensor([[0.4592]]) 0 0\n",
      "tensor([[0.5506]]) 0 1\n",
      "tensor([[0.4661]]) 0 0\n",
      "tensor([[0.3345]]) 0 0\n",
      "tensor([[0.1826]]) 0 0\n",
      "tensor([[0.1413]]) 0 0\n",
      "tensor([[0.2300]]) 0 0\n",
      "tensor([[0.0852]]) 0 0\n",
      "tensor([[0.8047]]) 1 1\n",
      "tensor([[0.1918]]) 0 0\n",
      "tensor([[0.3499]]) 0 0\n",
      "tensor([[0.4492]]) 1 0\n",
      "tensor([[0.6149]]) 0 1\n",
      "tensor([[0.4679]]) 0 0\n",
      "tensor([[0.5664]]) 0 1\n",
      "tensor([[0.2322]]) 1 0\n",
      "tensor([[0.3528]]) 1 0\n",
      "tensor([[0.3109]]) 1 0\n",
      "tensor([[0.8472]]) 1 1\n",
      "tensor([[0.5522]]) 1 1\n",
      "tensor([[0.4441]]) 0 0\n",
      "tensor([[0.1957]]) 0 0\n",
      "tensor([[0.5612]]) 0 1\n",
      "tensor([[0.2795]]) 0 0\n",
      "tensor([[0.5279]]) 1 1\n",
      "tensor([[0.7104]]) 1 1\n",
      "tensor([[0.3277]]) 0 0\n",
      "tensor([[0.1374]]) 0 0\n",
      "tensor([[0.4663]]) 0 0\n",
      "tensor([[0.4207]]) 0 0\n",
      "tensor([[0.3492]]) 0 0\n",
      "tensor([[0.6280]]) 0 1\n",
      "tensor([[0.1769]]) 0 0\n",
      "tensor([[0.7287]]) 0 1\n",
      "tensor([[0.2539]]) 0 0\n",
      "tensor([[0.3461]]) 0 0\n",
      "tensor([[0.4869]]) 0 0\n",
      "tensor([[0.7230]]) 1 1\n",
      "tensor([[0.4269]]) 0 0\n",
      "tensor([[0.5366]]) 0 1\n",
      "tensor([[0.6195]]) 1 1\n",
      "tensor([[0.4066]]) 0 0\n",
      "tensor([[0.3879]]) 1 0\n",
      "tensor([[0.3090]]) 1 0\n",
      "tensor([[0.1503]]) 0 0\n",
      "tensor([[0.2869]]) 0 0\n",
      "tensor([[0.3925]]) 0 0\n",
      "tensor([[0.7443]]) 0 1\n",
      "tensor([[0.0889]]) 0 0\n",
      "tensor([[0.4796]]) 0 0\n",
      "tensor([[0.3074]]) 1 0\n",
      "tensor([[0.2276]]) 0 0\n",
      "tensor([[0.4626]]) 1 0\n",
      "tensor([[0.5526]]) 1 1\n",
      "tensor([[0.4150]]) 0 0\n",
      "tensor([[0.4510]]) 1 0\n",
      "tensor([[0.2643]]) 0 0\n",
      "tensor([[0.3598]]) 0 0\n",
      "tensor([[0.1930]]) 1 0\n",
      "tensor([[0.5332]]) 0 1\n",
      "tensor([[0.5514]]) 0 1\n",
      "tensor([[0.3774]]) 0 0\n",
      "tensor([[0.8442]]) 1 1\n",
      "tensor([[0.1351]]) 0 0\n",
      "tensor([[0.7663]]) 1 1\n",
      "tensor([[0.1156]]) 0 0\n",
      "tensor([[0.3393]]) 0 0\n",
      "tensor([[0.2102]]) 0 0\n",
      "tensor([[0.1643]]) 1 0\n",
      "tensor([[0.4707]]) 1 0\n",
      "tensor([[0.3591]]) 0 0\n",
      "tensor([[0.7401]]) 1 1\n",
      "tensor([[0.3032]]) 0 0\n",
      "tensor([[0.5458]]) 0 1\n",
      "tensor([[0.3635]]) 0 0\n",
      "tensor([[0.4859]]) 1 0\n",
      "tensor([[0.6901]]) 0 1\n",
      "tensor([[0.1418]]) 0 0\n",
      "tensor([[0.4116]]) 1 0\n",
      "tensor([[0.2878]]) 0 0\n",
      "tensor([[0.3356]]) 0 0\n",
      "tensor([[0.5317]]) 0 1\n",
      "tensor([[0.3506]]) 0 0\n",
      "tensor([[0.3751]]) 1 0\n",
      "tensor([[0.3041]]) 0 0\n",
      "tensor([[0.3378]]) 0 0\n",
      "tensor([[0.2206]]) 0 0\n",
      "tensor([[0.5623]]) 0 1\n",
      "tensor([[0.0396]]) 0 0\n",
      "tensor([[0.6060]]) 0 1\n",
      "tensor([[0.3878]]) 1 0\n",
      "tensor([[0.6231]]) 0 1\n",
      "tensor([[0.8227]]) 1 1\n",
      "tensor([[0.3322]]) 0 0\n",
      "tensor([[0.4114]]) 0 0\n",
      "tensor([[0.4130]]) 1 0\n",
      "tensor([[0.3474]]) 0 0\n",
      "tensor([[0.4172]]) 0 0\n",
      "tensor([[0.2633]]) 0 0\n",
      "tensor([[0.9421]]) 1 1\n",
      "tensor([[0.3197]]) 0 0\n",
      "tensor([[0.7028]]) 1 1\n",
      "tensor([[0.6110]]) 0 1\n",
      "tensor([[0.2163]]) 0 0\n",
      "tensor([[0.3539]]) 1 0\n",
      "tensor([[0.7987]]) 1 1\n",
      "tensor([[0.1094]]) 0 0\n",
      "tensor([[0.4841]]) 0 0\n",
      "tensor([[0.1908]]) 0 0\n",
      "tensor([[0.2334]]) 0 0\n",
      "tensor([[0.2479]]) 0 0\n",
      "tensor([[0.7092]]) 1 1\n",
      "tensor([[0.1500]]) 0 0\n",
      "tensor([[0.2343]]) 0 0\n",
      "tensor([[0.3381]]) 0 0\n",
      "tensor([[0.2530]]) 0 0\n",
      "tensor([[0.7812]]) 1 1\n",
      "tensor([[0.1829]]) 0 0\n",
      "tensor([[0.2260]]) 0 0\n",
      "tensor([[0.3434]]) 0 0\n",
      "tensor([[0.2032]]) 0 0\n",
      "tensor([[0.4295]]) 0 0\n",
      "tensor([[0.5572]]) 1 1\n",
      "tensor([[0.2292]]) 0 0\n",
      "tensor([[0.2636]]) 1 0\n",
      "tensor([[0.1643]]) 1 0\n",
      "tensor([[0.3235]]) 0 0\n",
      "tensor([[0.5423]]) 1 1\n",
      "tensor([[0.3341]]) 0 0\n",
      "tensor([[0.4953]]) 0 0\n",
      "tensor([[0.8301]]) 0 1\n",
      "tensor([[0.4241]]) 1 0\n",
      "tensor([[0.3947]]) 0 0\n",
      "tensor([[0.5929]]) 1 1\n",
      "tensor([[0.0621]]) 0 0\n",
      "tensor([[0.1574]]) 0 0\n",
      "tensor([[0.3228]]) 0 0\n",
      "tensor([[0.5822]]) 0 1\n",
      "tensor([[0.3990]]) 0 0\n",
      "tensor([[0.2184]]) 0 0\n",
      "tensor([[0.1763]]) 0 0\n",
      "tensor([[0.3040]]) 0 0\n",
      "tensor([[0.3335]]) 0 0\n",
      "tensor([[0.1181]]) 0 0\n",
      "tensor([[0.6410]]) 0 1\n",
      "tensor([[0.2851]]) 0 0\n",
      "tensor([[0.3832]]) 1 0\n",
      "tensor([[0.1926]]) 0 0\n",
      "tensor([[0.5812]]) 1 1\n",
      "tensor([[0.2970]]) 0 0\n",
      "tensor([[0.6019]]) 0 1\n",
      "tensor([[0.3610]]) 0 0\n",
      "tensor([[0.3538]]) 0 0\n",
      "tensor([[0.4791]]) 0 0\n",
      "tensor([[0.3763]]) 0 0\n",
      "tensor([[0.5406]]) 1 1\n",
      "tensor([[0.1581]]) 0 0\n",
      "tensor([[0.4447]]) 0 0\n",
      "tensor([[0.5760]]) 1 1\n",
      "tensor([[0.2293]]) 0 0\n",
      "tensor([[0.4822]]) 0 0\n",
      "tensor([[0.3380]]) 0 0\n",
      "tensor([[0.1274]]) 0 0\n",
      "tensor([[0.5097]]) 0 1\n",
      "tensor([[0.2895]]) 0 0\n",
      "tensor([[0.2758]]) 0 0\n",
      "tensor([[0.3514]]) 0 0\n",
      "tensor([[0.4765]]) 0 0\n",
      "tensor([[0.1503]]) 0 0\n",
      "tensor([[0.3624]]) 0 0\n",
      "tensor([[0.5711]]) 0 1\n",
      "tensor([[-0.0616]]) 0 0\n",
      "tensor([[0.4847]]) 1 0\n",
      "tensor([[0.2168]]) 0 0\n",
      "tensor([[0.3348]]) 1 0\n",
      "tensor([[0.6648]]) 1 1\n",
      "tensor([[0.4091]]) 0 0\n",
      "tensor([[0.2856]]) 1 0\n",
      "tensor([[0.6362]]) 1 1\n",
      "tensor([[0.6334]]) 1 1\n",
      "tensor([[0.5544]]) 0 1\n",
      "tensor([[0.2097]]) 0 0\n",
      "tensor([[0.4837]]) 1 0\n",
      "tensor([[0.2205]]) 0 0\n",
      "tensor([[0.2895]]) 0 0\n",
      "tensor([[0.8454]]) 1 1\n",
      "tensor([[0.3048]]) 0 0\n",
      "tensor([[0.0674]]) 0 0\n",
      "tensor([[0.4195]]) 0 0\n",
      "tensor([[0.5761]]) 0 1\n",
      "tensor([[0.7735]]) 1 1\n",
      "tensor([[0.6791]]) 1 1\n",
      "tensor([[0.4111]]) 0 0\n",
      "tensor([[0.1518]]) 0 0\n",
      "tensor([[0.4087]]) 0 0\n",
      "tensor([[0.7585]]) 0 1\n",
      "tensor([[0.2662]]) 0 0\n",
      "tensor([[0.3997]]) 1 0\n",
      "tensor([[0.6095]]) 1 1\n",
      "tensor([[0.3202]]) 0 0\n",
      "tensor([[0.2716]]) 0 0\n",
      "tensor([[0.8287]]) 1 1\n",
      "tensor([[0.6735]]) 0 1\n",
      "tensor([[0.3781]]) 1 0\n",
      "tensor([[0.1724]]) 0 0\n",
      "tensor([[0.6869]]) 0 1\n",
      "tensor([[0.3949]]) 0 0\n",
      "tensor([[0.7655]]) 0 1\n",
      "tensor([[0.2296]]) 0 0\n",
      "tensor([[0.1706]]) 0 0\n",
      "tensor([[0.0877]]) 0 0\n",
      "tensor([[0.2592]]) 1 0\n",
      "tensor([[0.2868]]) 1 0\n",
      "tensor([[0.4904]]) 1 0\n",
      "tensor([[0.3881]]) 0 0\n",
      "tensor([[0.6814]]) 0 1\n",
      "tensor([[0.1657]]) 0 0\n",
      "tensor([[0.2636]]) 0 0\n",
      "tensor([[0.1725]]) 0 0\n",
      "tensor([[0.4399]]) 0 0\n",
      "tensor([[0.3196]]) 0 0\n",
      "tensor([[0.3265]]) 0 0\n",
      "tensor([[0.1107]]) 0 0\n",
      "tensor([[0.4996]]) 0 0\n",
      "tensor([[0.6506]]) 1 1\n",
      "tensor([[0.0899]]) 0 0\n",
      "tensor([[-0.0232]]) 0 0\n",
      "tensor([[0.2540]]) 0 0\n",
      "tensor([[0.4234]]) 0 0\n",
      "tensor([[0.2572]]) 0 0\n",
      "tensor([[0.2730]]) 0 0\n",
      "tensor([[0.3046]]) 0 0\n",
      "tensor([[0.6308]]) 1 1\n",
      "tensor([[0.5939]]) 0 1\n",
      "tensor([[0.2634]]) 0 0\n",
      "tensor([[0.6877]]) 1 1\n",
      "tensor([[0.2194]]) 0 0\n",
      "tensor([[0.4249]]) 0 0\n",
      "tensor([[0.8110]]) 1 1\n",
      "tensor([[0.2900]]) 0 0\n",
      "tensor([[0.4909]]) 0 0\n",
      "tensor([[0.2611]]) 0 0\n",
      "tensor([[0.0568]]) 0 0\n",
      "tensor([[0.0707]]) 0 0\n",
      "tensor([[0.2237]]) 0 0\n",
      "tensor([[0.1993]]) 1 0\n",
      "tensor([[0.0671]]) 0 0\n",
      "tensor([[0.3260]]) 1 0\n",
      "tensor([[0.4105]]) 0 0\n",
      "tensor([[0.6710]]) 0 1\n",
      "tensor([[0.4474]]) 1 0\n",
      "tensor([[0.2483]]) 0 0\n",
      "tensor([[0.1222]]) 0 0\n",
      "tensor([[0.6560]]) 0 1\n",
      "tensor([[0.5536]]) 1 1\n",
      "tensor([[0.4373]]) 0 0\n",
      "tensor([[0.5964]]) 0 1\n",
      "tensor([[0.2971]]) 0 0\n",
      "tensor([[0.5062]]) 0 1\n",
      "tensor([[0.5292]]) 0 1\n",
      "tensor([[0.3551]]) 0 0\n",
      "tensor([[0.2932]]) 0 0\n",
      "tensor([[0.4242]]) 0 0\n",
      "tensor([[0.5430]]) 1 1\n",
      "tensor([[0.2603]]) 1 0\n",
      "tensor([[0.0586]]) 0 0\n",
      "tensor([[0.1661]]) 0 0\n",
      "tensor([[0.7748]]) 0 1\n",
      "tensor([[0.4902]]) 0 0\n",
      "tensor([[0.6078]]) 0 1\n",
      "tensor([[0.3617]]) 0 0\n",
      "tensor([[0.4166]]) 0 0\n",
      "tensor([[0.3378]]) 1 0\n",
      "tensor([[-0.0276]]) 0 0\n",
      "tensor([[0.1555]]) 0 0\n",
      "tensor([[0.3859]]) 0 0\n",
      "tensor([[0.2930]]) 0 0\n",
      "tensor([[0.7454]]) 1 1\n",
      "tensor([[0.1738]]) 0 0\n",
      "tensor([[0.6760]]) 1 1\n",
      "tensor([[0.5607]]) 0 1\n",
      "tensor([[0.1356]]) 0 0\n",
      "tensor([[0.5185]]) 1 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\D SAIPAVAN KUMAR\\Downloads\\Semantic-Similarity--main\\BCS_Head_task\\final.ipynb Cell 11\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/D%20SAIPAVAN%20KUMAR/Downloads/Semantic-Similarity--main/BCS_Head_task/final.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_list)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/D%20SAIPAVAN%20KUMAR/Downloads/Semantic-Similarity--main/BCS_Head_task/final.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     embedding1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(test_list[i][\u001b[39m'\u001b[39m\u001b[39mtexts\u001b[39m\u001b[39m'\u001b[39m], convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/D%20SAIPAVAN%20KUMAR/Downloads/Semantic-Similarity--main/BCS_Head_task/final.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     embedding2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(test_list[i][\u001b[39m'\u001b[39;49m\u001b[39mreason\u001b[39;49m\u001b[39m'\u001b[39;49m], convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/D%20SAIPAVAN%20KUMAR/Downloads/Semantic-Similarity--main/BCS_Head_task/final.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# compute similarity scores of two embeddings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/D%20SAIPAVAN%20KUMAR/Downloads/Semantic-Similarity--main/BCS_Head_task/final.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     cosine_scores \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mpytorch_cos_sim(embedding1, embedding2)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrans_features, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:853\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    844\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    846\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    847\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    848\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    852\u001b[0m )\n\u001b[1;32m--> 853\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    854\u001b[0m     embedding_output,\n\u001b[0;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    857\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    858\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    859\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    860\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    861\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    862\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    864\u001b[0m )\n\u001b[0;32m    865\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    866\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:454\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    451\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    452\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 454\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    455\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    457\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    459\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:467\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    466\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 467\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    468\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:378\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 378\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    379\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    380\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(test_list)):\n",
    "    embedding1 = model.encode(test_list[i]['texts'], convert_to_tensor=True)\n",
    "    embedding2 = model.encode(test_list[i]['reason'], convert_to_tensor=True)\n",
    "# compute similarity scores of two embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    if cosine_scores > 0.5:\n",
    "        test_list[i]['pred'] = 1\n",
    "    else:\n",
    "        test_list[i]['pred'] = 0\n",
    "\n",
    "    print(cosine_scores, test_list[i]['label'], test_list[i]['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [l['label'] for l in test_list[:700]]\n",
    "y_pred = [l['pred'] for l in test_list[:700]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.55\n",
      "Recall: 0.50\n",
      "F1 score: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are arrays of binary labels (0 or 1)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(100):\n",
    "    sum = (test_list[i]['label'] == test_list[i]['pred']) + sum\n",
    "    # if (test_list[i]['label'] != test_list[i]['pred']):\n",
    "    #     print(f\"text: {test_list[i]['texts']}, reason: {test_list[i]['reason']}\")\n",
    "    #     print(f\"labelled: {test_list[i]['label']}, predicted: {test_list[i]['pred']}\")\n",
    "accuracy = sum/100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
